{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code below downloads the CIFAR-10 dataset\n",
    "\"\"\"\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):#Progress Bar\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):#Download dataset\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consist of 10 objects in 60000 32x32 images\n",
    "The 10 objects are:\n",
    "* Airplane\n",
    "* Automobile\n",
    "* Bird\n",
    "* Cat\n",
    "* Deer\n",
    "* Dog\n",
    "* Frog\n",
    "* Horse\n",
    "* Ship\n",
    "* Truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as ply\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color doesn't matter when trying to classify images.\n",
    "\n",
    "![Which is easier to classify](img/color.PNG)\n",
    "\n",
    "Likewise neural networks don't know how to classify names, it cannot tell whether a picture is an airplane or a dog.\n",
    "Instead we are going to transform them into vectors which look like this\n",
    "\n",
    "![Labels -> Vectors](img/one-hot.PNG)\n",
    "\n",
    "## How do we calculate how far off we were? What is our Error?\n",
    "\n",
    "![Categorical Cross Entropy](img/cross-entropy.PNG)\n",
    "\n",
    "## How do we Learn?\n",
    "\n",
    "![Gradient Descent](img/GD.PNG)\n",
    "\n",
    "Using gradient descent we can minimize our error, but for images its not practical to calculate.\n",
    "\n",
    "### Stocastic Gradient Descent\n",
    "\n",
    "![Stocastic Gradient Descent](img/SGD.PNG)\n",
    "\n",
    "Instead lets take a subset of our error and \"guess\" the right direction to minimize our error. Less calculations but a lot more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(images):\n",
    "    \"\"\"\n",
    "    Turn a color image into grayscale\n",
    "    Colors are 3 dimensional, ranging from 0-255 for RGB\n",
    "    \"\"\"\n",
    "    return NotImplemented\n",
    "\n",
    "\n",
    "def one_hot_encoding(labels):\n",
    "    \"\"\"\n",
    "    Transform labels from word description\n",
    "    to vectors\n",
    "    \"\"\"\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing our dataset\n",
    "\n",
    "## We are going to remove the color from the images using our normalize function and than store the labels as vectors through one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "Set up files for batching\n",
    "\"\"\"\n",
    "\n",
    "def batches(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "    \"\"\"\n",
    "    Preprocess Training and Validation Data\n",
    "    \"\"\"\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
    "        validation_count = int(len(features) * 0.1)\n",
    "\n",
    "        # Process and save a batch of training data\n",
    "        preprocess_and_save(\n",
    "            normalize,\n",
    "            one_hot_encode,\n",
    "            features[:-validation_count],\n",
    "            labels[:-validation_count],\n",
    "            'preprocess_batch_' + str(batch_i) + '.p')\n",
    "\n",
    "        # Use a portion of training batch for validation\n",
    "        valid_features.extend(features[-validation_count:])\n",
    "        valid_labels.extend(labels[-validation_count:])\n",
    "\n",
    "    # Preprocess and Save all validation data\n",
    "    preprocess_and_save(\n",
    "        normalize,\n",
    "        one_hot_encode,\n",
    "        np.array(valid_features),\n",
    "        np.array(valid_labels),\n",
    "        'preprocess_validation.p')\n",
    "\n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    # load the test data\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    test_labels = batch['labels']\n",
    "\n",
    "    # Preprocess and Save all test data\n",
    "    preprocess_and_save(\n",
    "        normalize,\n",
    "        one_hot_encode,\n",
    "        np.array(test_features),\n",
    "        np.array(test_labels),\n",
    "        'preprocess_test.p')\n",
    "    \n",
    "def preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "    \"\"\"\n",
    "    Preprocess data and save it to file\n",
    "    \"\"\"\n",
    "    features = normalize(features)\n",
    "    labels = one_hot_encode(labels)\n",
    "\n",
    "    pickle.dump((features, labels), open(filename, 'wb'))\n",
    "    \n",
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    \"\"\"\n",
    "    Load a batch of the dataset\n",
    "    \"\"\"\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "\n",
    "    return features, labels    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching\n",
    "\n",
    "## With GPU programming we have limited space, sometimes datasets are to large to hold in memory\n",
    "\n",
    "## To overcome this we batch our data into multiple training sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the batches\n",
    "batches(cifar10_dataset_folder_path, normalize, one_hot_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Now that we have our data save, we are gonna load it and train on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import problem_unittests as tests\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)\n",
    "\n",
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow syntax\n",
    "## Basic data strctures and variables\n",
    "- tf.placeholder(tf.typeID, shape, 'name') : type placeholder. shape and name are optional/ No shape implies single variable\n",
    "- tf.Variable(initial-value, 'name') : variable placeholder\n",
    "- tf.zeros(shape, 'name') : Initialize a tensor of zeros\n",
    "- tf.truncated_normal(shape, mean, stddev) : will generate numbers from a random normal distribution using a given mean and standard deviation\n",
    "- tf.nn.conv2d(tensor, weights, stride, padding) : weights and tensor must be of matching dimension, stride is the amount of data which is read in, padding can either be same or valid. \n",
    "- tf.nn.max_pool(tensor, kernel size, stride, padding) : Performs max-pooling with given kernel size and stride\n",
    "- tf.contrib.layers.flatten(tensor) : Flattens a 4-D tensor to a 2-D tensor\n",
    "- tf.contrib.layers.fully_connected(tensor, size) : Creates a fully connected layer of a given size\n",
    "- tf.nn.dropout(tensor, probability) : performs dropout on a tensor\n",
    "\n",
    "When giving None to tensorflow, it means that we are specifying a dynamic size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Input\n",
    "\n",
    "## We need to create placeholders for the images, labels, and our probability to keep our data\n",
    "\n",
    "# Dropout\n",
    "\n",
    "### One problem common with supervised learning algorithm is overfitting. We can overcome this by using dropout\n",
    "\n",
    "### when will randomly 'turn off' parts of the network, never allowing for the Neural Network to fully learn the images. We want it to learn the shapes that are associated with the objects\n",
    "\n",
    "![Stocastic Gradient Descent](img/dropout.PNG)\n",
    "\n",
    "## Creating our Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, (None ,) + image_shape, 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.placeholder(tf.float32, (None, n_classes) , 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "tf.reset_default_graph()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Convolutions and Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(inputs, filter_size, ksize, kstrides, padding, activation):\n",
    "    \"\"\"\n",
    "    Apply Convolution to a Tensor\n",
    "    :param inputs: Tensorflow Tensor\n",
    "    :param filter_size: How deep the filter \"weights\" are\n",
    "    :param ksize: Size of the kernel\n",
    "    :param kstrides: How fast we slide the kernel\n",
    "    :param padding: 'SAME' or 'VALID'\n",
    "    :param activation: RELU\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([None], mean=None, stddev=None))\n",
    "    bias = tf.Variable(tf.zeros([None]))\n",
    "    tensor = tf.nn.conv2d(inputs, weights, \n",
    "                 strides=[1] + list(kstrides) + [1], \n",
    "                 padding=padding)\n",
    "    \n",
    "    tensor = tf.nn.bias_add(tensor, bias)\n",
    "    return activation(tensor)\n",
    "\n",
    "def conv2d_maxpool(inputs, pool_size, strides):\n",
    "    \"\"\"\n",
    "    Apply max pooling on the input tensor\n",
    "    :param Inputs: TensorFlow Tensor\n",
    "    :param pool_size: kernel size that we are sliding across\n",
    "    :param strides: Sliding speed\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.nn.max_pool(inputs, ksize=[1] + list(pool_size) + [1], strides = [1] + list(strides) + [1], padding='SAME')\n",
    "\n",
    "\n",
    "def output(inputs, units):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    weight = tf.Variable(tf.truncated_normal([inputs.get_shape()[1].value, units], mean=0, stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros([units]))\n",
    "    \n",
    "    out = tf.matmul(inputs, weight)\n",
    "    out = tf.nn.bias_add(out, bias)\n",
    "    return out\n",
    "\n",
    "tf.reset_default_graph()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing VGG-19\n",
    "\n",
    "## first we are going to take a look at LeNet\n",
    "\n",
    "![LeNet](img/LeNet.PNG)\n",
    "\n",
    "The output of our last layer, using the output function, is known as a logit.\n",
    "\n",
    "# Our learning function, also known as an activation function, is called Relu\n",
    "\n",
    "## Relu : Rectified Linear Units\n",
    "\n",
    "We use Relu over the sigmoid function for multiple reasons. The most important one for now is that Relu are significantly faster to calculate versus sigmoid\n",
    "\n",
    "![relu](img/relu.PNG)\n",
    "\n",
    "# Our neural network will predict a vector with 10 values\n",
    "\n",
    "## We want these values to be a percentage, one function which can create a percentage out of values is known as softmax\n",
    "\n",
    "![soft](img/softmax.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    conv_ksize = [None, None]\n",
    "    conv_strides = [None, None]\n",
    "    pool_ksize = [None, None]\n",
    "    pool_strides = [None, None]\n",
    "    \n",
    "    \n",
    "    return NotImplemented\n",
    "    \n",
    "tf.reset_default_graph()    \n",
    "\n",
    "inputs = neural_net_image_input((32,32,3)) #size of the images\n",
    "labels = neural_net_label_input(10) #amount of labels\n",
    "keep_prob = neural_net_keep_prob_input() #probability for dropout\n",
    "\n",
    "logits = conv_net(inputs, keep_prob)\n",
    "\n",
    "#Saving variables for use later\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "#Cost and gradient decent\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we are going to train our neural network\n",
    "\n",
    "## We are also going to print out how well we do with each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict = {inputs: feature_batch, labels: label_batch, keep_prob: keep_probability})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={inputs: feature_batch, labels: label_batch, keep_prob: 1})\n",
    "    acc = session.run(accuracy, feed_dict={inputs: valid_features, labels: valid_labels, keep_prob: 1})\n",
    "    \n",
    "    print ('Loss: {:>10.4f} Validation: {:.6f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are many parameters involved when building neural networks\n",
    "- epochs : The amount of times we feed the entire dataset to the network\n",
    "- batch size : How fast our network will learn. The better the GPU the larger this value can be. Traditionally in powers of 2\n",
    "- Keep probability: How much of the network we want to randomly turn off at any given time\n",
    "\n",
    "We are also going to save our model, we don't want to train our network each time we get new data. With tensorflow you can record the weights and biases you learned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "keep_probability = 0.8\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well did we do?\n",
    "\n",
    "## This was our accuracy against the training and validation set, now we will ask it to predict the label for images it has never seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to help us print our predictions\n",
    "def display_image_predictions(features, labels, predictions):\n",
    "    n_classes = 10\n",
    "    label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "\n",
    "    fig, axies = plt.subplots(nrows=4, ncols=2)\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
    "\n",
    "    n_predictions = 3\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "\n",
    "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
    "        pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "        correct_name = label_names[label_id]\n",
    "\n",
    "        axies[image_i][0].imshow(feature)\n",
    "        axies[image_i][0].set_title(correct_name)\n",
    "        axies[image_i][0].set_axis_off()\n",
    "\n",
    "        axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
    "        axies[image_i][1].set_yticks(ind + margin)\n",
    "        axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
    "        axies[image_i][1].set_xticks([0, 0.5, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to import the model we just made and use the testing set to see how well we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlnd-tf-lab]",
   "language": "python",
   "name": "conda-env-dlnd-tf-lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
